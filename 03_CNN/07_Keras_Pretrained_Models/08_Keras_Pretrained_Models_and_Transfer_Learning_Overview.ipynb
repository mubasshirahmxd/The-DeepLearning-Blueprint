{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f46b5c",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;\">Keras Pretrained Models & Transfer Learning — Practical Overview</h2>\n",
    "\n",
    "**Author:** Mubasshir Ahmed  \n",
    "**Module:** Deep Learning — FSDS  \n",
    "**Notebook:** 08_Keras_Pretrained_Models_and_Transfer_Learning_Overview  \n",
    "**Objective:** Recreate and expand the content from `KERAS PKGS.pdf` into a complete, runnable notebook that demonstrates loading Keras pretrained models, preprocessing, running inference, comparing performance, and explaining transfer learning concepts and best practices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1855d369",
   "metadata": {},
   "source": [
    "### 1️⃣ Introduction — Why Pretrained Models & Transfer Learning?\n",
    "\n",
    "- **Transfer Learning** allows us to leverage models pretrained on large datasets (like ImageNet) and adapt them to new, smaller tasks.  \n",
    "- Benefits:\n",
    "  - Saves time and compute (no need to train from scratch).\n",
    "  - Often achieves better generalization on small datasets.\n",
    "  - Useful for feature extraction (use base as fixed feature map) or fine-tuning (unfreeze some layers).\n",
    "- This notebook follows and expands the demos in your PDF: we'll load multiple Keras application models, preprocess a sample image correctly for each, measure inference time and parameter counts, and show how to set up a transfer-learning training pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ Imports and paths (Windows-safe, dynamic)\n",
    "import os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# models and preprocessors we'll use\n",
    "from tensorflow.keras.applications import (\n",
    "    vgg16, resnet50, inception_v3, mobilenet_v2, efficientnet\n",
    ")\n",
    "\n",
    "print(\"# ➤ TensorFlow / Keras version:\", keras.__version__)\n",
    "\n",
    "# Paths: expect notebook placed inside folder containing 'sample_image/2.jpg'\n",
    "base_dir = os.getcwd()\n",
    "sample_dir = os.path.join(base_dir, 'sample_image')\n",
    "img_path = os.path.join(sample_dir, '2.jpg')\n",
    "if not os.path.exists(img_path):\n",
    "    raise FileNotFoundError(f\"Sample image not found: {img_path}. Place a JPG at sample_image/2.jpg\")\n",
    "\n",
    "print('# ➤ sample image:', img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc95156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()  # ➤ choose your 2.jpg file from sample_image folder\n",
    "img_path = '2.jpg'\n",
    "print('# ➤ sample image:', img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facb7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3️⃣ Display sample image\n",
    "img = load_img(img_path)\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('Sample image (for model demos)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd777762",
   "metadata": {},
   "source": [
    "### 4️⃣ Preprocessing notes\n",
    "\n",
    "Different pretrained models expect different input sizes and preprocessing functions (`preprocess_input`).  \n",
    "Keras provides model-specific preprocessors in each application module (e.g., `vgg16.preprocess_input`, `resnet50.preprocess_input`, etc.).  \n",
    "Common input sizes:\n",
    "- VGG16, ResNet50, MobileNetV2, EfficientNetB0: **224×224**\n",
    "- InceptionV3: **299×299**\n",
    "\n",
    "We'll prepare a helper function that resizes, converts to array, expands dims and applies the correct `preprocess_input` for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac9e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Helper: prepare image for model\n",
    "def prepare_image(img_path, target_size, preprocess_fn):\n",
    "    # load image, resize\n",
    "    img = load_img(img_path, target_size=target_size)\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_fn(x)\n",
    "    return x\n",
    "\n",
    "# Quick sanity test for VGG preprocess\n",
    "x_vgg = prepare_image(img_path, target_size=(224,224), preprocess_fn=vgg16.preprocess_input)\n",
    "print('# ➤ Prepared VGG input shape:', x_vgg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b148c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5️⃣ Helper: load model, run inference, and summarize\n",
    "def run_model_demo(name, model_loader, preprocess_fn, target_size=(224,224), decode_fn=None, top=3):\n",
    "    print('\\n' + '='*40)\n",
    "    print(f'# ➤ Model: {name}')\n",
    "    # load model (may download weights first time)\n",
    "    start = time.time()\n",
    "    model = model_loader(weights='imagenet', include_top=True, input_shape=(*target_size,3))\n",
    "    load_time = time.time() - start\n",
    "    print(f'Loaded model in {load_time:.2f}s')\n",
    "    # model size and params\n",
    "    total_params = model.count_params()\n",
    "    trainable = np.sum([keras.backend.count_params(p) for p in model.trainable_weights])\n",
    "    non_trainable = np.sum([keras.backend.count_params(p) for p in model.non_trainable_weights])\n",
    "    print(f'Total params: {total_params:,} | Trainable: {trainable:,} | Non-trainable: {non_trainable:,}')\n",
    "    # prepare input\n",
    "    x = prepare_image(img_path, target_size=target_size, preprocess_fn=preprocess_fn)\n",
    "    # inference timing\n",
    "    t0 = time.time()\n",
    "    preds = model.predict(x)\n",
    "    t1 = time.time()\n",
    "    inf_time = t1 - t0\n",
    "    print(f'Inference time: {inf_time:.4f} s')\n",
    "    # decode predictions if provided\n",
    "    if decode_fn is not None:\n",
    "        decoded = decode_fn(preds, top=top)[0]\n",
    "        print('Top predictions:')\n",
    "        for cls, desc, prob in decoded:\n",
    "            print(f' - {desc}: {prob:.4f}')\n",
    "    return {'name': name, 'model': model, 'total_params': total_params, 'trainable': trainable, 'non_trainable': non_trainable, 'load_time': load_time, 'inference_time': inf_time}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a35bff",
   "metadata": {},
   "source": [
    "### 5️⃣ Demo: Load models and run inference\n",
    "\n",
    "We'll run demos for:\n",
    "- VGG16\n",
    "- ResNet50\n",
    "- InceptionV3\n",
    "- MobileNetV2\n",
    "- EfficientNetB0\n",
    "\n",
    "This will show loading time, parameter counts, inference time and top predictions on the sample image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Run VGG16 demo\n",
    "vgg_res = run_model_demo('VGG16', vgg16.VGG16, vgg16.preprocess_input, target_size=(224,224), decode_fn=vgg16.decode_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Run ResNet50 demo\n",
    "resnet_res = run_model_demo('ResNet50', resnet50.ResNet50, resnet50.preprocess_input, target_size=(224,224), decode_fn=resnet50.decode_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897234c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Run InceptionV3 demo (uses 299x299)\n",
    "incept_res = run_model_demo('InceptionV3', inception_v3.InceptionV3, inception_v3.preprocess_input, target_size=(299,299), decode_fn=inception_v3.decode_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88436282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Run MobileNetV2 demo\n",
    "mobilenet_res = run_model_demo('MobileNetV2', mobilenet_v2.MobileNetV2, mobilenet_v2.preprocess_input, target_size=(224,224), decode_fn=mobilenet_v2.decode_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 Run EfficientNetB0 demo\n",
    "effnet_res = run_model_demo('EfficientNetB0', efficientnet.EfficientNetB0, efficientnet.preprocess_input, target_size=(224,224), decode_fn=efficientnet.decode_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a3f20f",
   "metadata": {},
   "source": [
    "### 6️⃣ Comparison Summary\n",
    "\n",
    "Below we collect the basic metrics (total params, trainable params, inference time) for each model we ran.  \n",
    "These numbers help choose a model depending on constraints (compute, latency, accuracy needs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaa93e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Build comparison table\n",
    "results = [vgg_res, resnet_res, incept_res, mobilenet_res, effnet_res]\n",
    "import pandas as pd\n",
    "rows = []\n",
    "for r in results:\n",
    "    rows.append({\n",
    "        'Model': r['name'],\n",
    "        'Total Params': r['total_params'],\n",
    "        'Trainable Params': r['trainable'],\n",
    "        'Non-trainable Params': r['non_trainable'],\n",
    "        'Load Time (s)': round(r['load_time'],2),\n",
    "        'Inference Time (s)': round(r['inference_time'],4)\n",
    "    })\n",
    "df = pd.DataFrame(rows).sort_values('Total Params', ascending=False).reset_index(drop=True)\n",
    "df['Total Params'] = df['Total Params'].apply(lambda x: f\"{x:,}\")\n",
    "df['Trainable Params'] = df['Trainable Params'].apply(lambda x: f\"{x:,}\")\n",
    "df['Non-trainable Params'] = df['Non-trainable Params'].apply(lambda x: f\"{x:,}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa61f225",
   "metadata": {},
   "source": [
    "### 7️⃣ How to use these pretrained models for Transfer Learning\n",
    "\n",
    "**Two common strategies:**\n",
    "\n",
    "1. **Feature Extraction (recommended first):**\n",
    "   - Load the pretrained model **without** the top classification layers: `include_top=False`.\n",
    "   - Freeze the base (`base.trainable = False`).\n",
    "   - Add a small classification head (GlobalAveragePooling2D, Dense, Dropout, Dense output) and train only the head.\n",
    "\n",
    "2. **Fine-tuning (after feature extraction):**\n",
    "   - Unfreeze some top layers of the base model and train with a **very low learning rate** (e.g., 1e-5).\n",
    "   - Helps adapt higher-level features to your specific task.\n",
    "\n",
    "**Notes:**\n",
    "- Use `preprocess_input` for the chosen model.\n",
    "- Use `ImageDataGenerator` for on-the-fly augmentation.\n",
    "- Use callbacks: `ModelCheckpoint`, `EarlyStopping`, `ReduceLROnPlateau`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8️⃣ Example: Feature extraction with MobileNetV2 (include_top=False)\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "IMG_SIZE = (224,224)\n",
    "base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(*IMG_SIZE,3))\n",
    "base.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c048361",
   "metadata": {},
   "source": [
    "#### Fine-tuning example (unfreeze top layers)\n",
    "After training the head, unfreeze the top N layers and continue training with a lower learning rate:\n",
    "\n",
    "```python\n",
    "base.trainable = True\n",
    "for layer in base.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=10, callbacks=callbacks)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6564ec76",
   "metadata": {},
   "source": [
    "### 9️⃣ Practical Tips & Model Selection Guidelines\n",
    "\n",
    "- **MobileNetV2** / **EfficientNetB0**: Good starting points for laptop/Colab — fast and efficient.  \n",
    "- **ResNet50**: Solid general-purpose model if you have a GPU.  \n",
    "- **VGG16**: Simple to understand but heavy; avoid for deployment.  \n",
    "- **InceptionV3/Xception**: Useful for accuracy; requires larger input size and more compute.  \n",
    "- Always monitor validation performance and use early stopping.  \n",
    "- If dataset is tiny, prefer feature extraction + augmentation; only fine-tune if you have more data or validation improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10️⃣ Appendix: helper to build base model by name (include_top option)\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, InceptionV3, MobileNetV2, EfficientNetB0\n",
    "\n",
    "def get_base_model(name='mobilenet', img_size=(224,224), include_top=False):\n",
    "    name = name.lower()\n",
    "    if name == 'vgg16':\n",
    "        return VGG16(weights='imagenet', include_top=include_top, input_shape=(*img_size,3))\n",
    "    if name == 'resnet50':\n",
    "        return ResNet50(weights='imagenet', include_top=include_top, input_shape=(*img_size,3))\n",
    "    if name == 'inceptionv3':\n",
    "        return InceptionV3(weights='imagenet', include_top=include_top, input_shape=(*img_size,3))\n",
    "    if name == 'mobilenetv2':\n",
    "        return MobileNetV2(weights='imagenet', include_top=include_top, input_shape=(*img_size,3))\n",
    "    if name == 'efficientnetb0':\n",
    "        return EfficientNetB0(weights='imagenet', include_top=include_top, input_shape=(*img_size,3))\n",
    "    raise ValueError('Unknown model name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae14ee7b",
   "metadata": {},
   "source": [
    "### ✅ Wrap-up\n",
    "\n",
    "This notebook reproduced and extended the demos from your PDF:\n",
    "- Loading pretrained Keras models\n",
    "- Model-specific preprocessing and inference\n",
    "- Measuring parameter counts and inference speed\n",
    "- Demonstrating how to use models for transfer learning (feature extraction + fine-tuning)\n",
    "\n",
    "**Next practical notebook:** we'll apply the MobileNetV2 transfer-learning recipe on a real dataset (or the Happy/Not Happy project) if you want — that will include generators, augmentation, training, fine-tuning, and evaluation, end-to-end.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
