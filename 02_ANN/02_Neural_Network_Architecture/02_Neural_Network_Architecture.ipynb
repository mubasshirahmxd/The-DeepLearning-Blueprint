{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3004b662",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;\">Neural Network Architecture</h2>\n",
    "\n",
    "**Author:** Mubasshir Ahmed**  \n",
    "**Module:** Deep Learning ‚Äî FSDS  \n",
    "**Notebook:** 02_Neural_Network_Architecture  \n",
    "**Objective:** Understand how a Neural Network is structured, how data flows through it, and how its components interact during learning.\n",
    "\n",
    "---\n",
    "\n",
    "A **Neural Network Architecture** defines the **arrangement of layers, neurons, and connections** that transform raw data into meaningful predictions.  \n",
    "It determines how the network learns, how complex patterns can be recognized, and how efficiently it performs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a76ed31",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">1Ô∏è‚É£ What is Neural Network Architecture?</h3>\n",
    "\n",
    "A **neural network architecture** is like the *blueprint* of a brain-inspired machine ‚Äî it specifies how neurons are connected and how information flows between them.\n",
    "\n",
    "Each network has:\n",
    "- A **defined number of layers** (input, hidden, output)\n",
    "- A **specific number of neurons** per layer\n",
    "- **Weights** and **biases** for every connection\n",
    "- **Activation functions** that introduce non-linearity\n",
    "\n",
    "**Analogy:**  \n",
    "> Think of a neural network as a multi-floor building:  \n",
    "> - Each floor = a layer  \n",
    "> - Each room = a neuron  \n",
    "> - Wires connecting rooms = weights  \n",
    "> - Power switches controlling flow = activation functions  \n",
    "> Data enters from the ground floor and exits the top floor after transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89f71e",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">2Ô∏è‚É£ Layers in a Neural Network</h3>\n",
    "\n",
    "There are three main types of layers in an ANN:\n",
    "\n",
    "| Layer | Function | Example |\n",
    "|--------|-----------|----------|\n",
    "| **Input Layer** | Receives raw input data | Pixels of an image, features in dataset |\n",
    "| **Hidden Layers** | Transform and extract patterns | Intermediate representations |\n",
    "| **Output Layer** | Produces final prediction | Classification label or numeric value |\n",
    "\n",
    "- The number of **neurons in the input layer** equals the number of features in your dataset.  \n",
    "- The **output layer neurons** depend on the problem type:  \n",
    "  - Regression ‚Üí 1 neuron  \n",
    "  - Binary classification ‚Üí 1 neuron (sigmoid)  \n",
    "  - Multiclass ‚Üí n neurons (softmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de4d0ea",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">3Ô∏è‚É£ How Information Flows ‚Äî Feedforward Path</h3>\n",
    "\n",
    "Information in a neural network flows **forward** through connections between neurons.\n",
    "\n",
    "Each neuron in one layer connects to every neuron in the next layer (fully connected).  \n",
    "\n",
    "**Computation per neuron:**  \n",
    "\\[ z_j = \\sum_i w_{ij}x_i + b_j \\]  \n",
    "\\[ a_j = f(z_j) \\]\n",
    "\n",
    "Where:  \n",
    "- \\( x_i \\) = input from previous layer  \n",
    "- \\( w_{ij} \\) = weight of connection  \n",
    "- \\( b_j \\) = bias term  \n",
    "- \\( f(z) \\) = activation function  \n",
    "- \\( a_j \\) = output of neuron *j*\n",
    "\n",
    "This process is repeated across all layers until the output is produced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36077289",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">4Ô∏è‚É£ Weights and Bias ‚Äî The Core of Learning</h3>\n",
    "\n",
    "- **Weights (W):** Control the influence of one neuron over another.  \n",
    "  Higher weights ‚Üí stronger influence.\n",
    "\n",
    "- **Bias (b):** Allows flexibility in shifting the activation threshold.  \n",
    "  Without bias, all activations would pass through the origin, limiting learning.\n",
    "\n",
    "**Example:**  \n",
    "For a simple neuron:  \n",
    "\\[ y = f(w_1x_1 + w_2x_2 + b) \\]\n",
    "\n",
    "During training, the weights and biases get adjusted to minimize prediction errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7c72b1",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">5Ô∏è‚É£ Activation Functions in Architecture</h3>\n",
    "\n",
    "Activation functions define **when** and **how much** a neuron should activate.\n",
    "\n",
    "| Function | Role | Output Range |\n",
    "|-----------|------|--------------|\n",
    "| **ReLU** | Prevents vanishing gradients, speeds up learning | [0, ‚àû) |\n",
    "| **Sigmoid** | Used in binary classification | (0, 1) |\n",
    "| **Tanh** | Smooth symmetric output | (-1, 1) |\n",
    "| **Softmax** | Multi-class probability distribution | (0, 1) |\n",
    "\n",
    "**Tip:** Hidden layers commonly use ReLU; output layer depends on the task type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b3b46",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">6Ô∏è‚É£ Forward Propagation (Mathematical View)</h3>\n",
    "\n",
    "**Step-by-step example:**\n",
    "\n",
    "1. Input vector \\( X = [x_1, x_2, x_3] \\) enters the network.  \n",
    "2. Each hidden layer neuron computes weighted input and adds bias:  \n",
    "   \\[ z = W \\cdot X + b \\]\n",
    "3. Activation function transforms z ‚Üí a (output activation).  \n",
    "4. The output of this layer becomes input to the next layer.  \n",
    "5. Final output layer produces prediction \\( \\hat{y} \\).\n",
    "\n",
    "This process is known as **feedforward propagation**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c568ddeb",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">7Ô∏è‚É£ Backward Propagation (Weight Adjustment)</h3>\n",
    "\n",
    "Once the network predicts \\( \\hat{y} \\), it compares it with the true value \\( y \\) using a **loss function**.\n",
    "\n",
    "The error is then propagated **backward** to adjust the weights using the **chain rule** ‚Äî this is called **backpropagation**.\n",
    "\n",
    "**Formula:**  \n",
    "\\[ W_{new} = W_{old} - \\eta \\frac{\\partial L}{\\partial W} \\]\n",
    "\n",
    "Where:  \n",
    "- \\( L \\) = loss function  \n",
    "- \\( \\eta \\) = learning rate  \n",
    "- \\( \\partial L/\\partial W \\) = gradient of loss w.r.t weight\n",
    "\n",
    "**Goal:** Reduce the loss step by step until the model learns optimal parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b76cdd6",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">8Ô∏è‚É£ Hyperparameters That Define Architecture</h3>\n",
    "\n",
    "| Hyperparameter | Description | Typical Range |\n",
    "|-----------------|--------------|----------------|\n",
    "| **Number of Hidden Layers** | Controls model depth | 1‚Äì5 (ANN) |\n",
    "| **Neurons per Layer** | Affects model capacity | 8‚Äì512 |\n",
    "| **Learning Rate** | Step size in weight updates | 0.1 ‚Üí 0.0001 |\n",
    "| **Batch Size** | Samples per gradient update | 16‚Äì128 |\n",
    "| **Epochs** | Number of complete training cycles | 10‚Äì500 |\n",
    "| **Optimizer** | Controls how gradients update weights | SGD, Adam, RMSProp |\n",
    "\n",
    "Choosing these wisely prevents underfitting or overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f846f3",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">9Ô∏è‚É£ Designing a Good ANN Architecture</h3>\n",
    "\n",
    "Guidelines for building effective architectures:\n",
    "\n",
    "‚úÖ Start simple ‚Äî 1 or 2 hidden layers, 16‚Äì64 neurons each.  \n",
    "‚úÖ Use **ReLU** in hidden layers, **Sigmoid/Softmax** in output.  \n",
    "‚úÖ Normalize input data for faster convergence.  \n",
    "‚úÖ Experiment with learning rates (0.01‚Äì0.001).  \n",
    "‚úÖ Regularize with dropout for large networks.  \n",
    "‚úÖ Always monitor training vs validation accuracy/loss.\n",
    "\n",
    "> Simplicity first ‚Äî then add complexity if the model underfits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc538c39",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">üîü Summary & Takeaways</h3>\n",
    "\n",
    "- Neural Network Architecture defines the arrangement of layers and neurons.  \n",
    "- Data flows **forward** for prediction and **backward** for learning.  \n",
    "- **Weights** and **biases** determine how strongly neurons interact.  \n",
    "- **Activation functions** make learning non-linear and powerful.  \n",
    "- Proper choice of **hyperparameters** determines success or failure.\n",
    "\n",
    "**Next:** Proceed to `03_Forward_Backward_Propagation/` to explore mathematical details of learning and gradient computation.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
