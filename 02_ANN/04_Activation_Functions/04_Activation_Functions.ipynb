{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ba84ae3",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;\">Activation Functions ‚Äî Deep Learning</h2>\n",
    "\n",
    "**Author:** Mubasshir Ahmed  \n",
    "**Module:** Deep Learning ‚Äî FSDS  \n",
    "**Notebook:** 04_Activation_Functions  \n",
    "**Objective:** Explain activation functions, why non-linearity is essential, and how to choose activations for different layers and tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3876e9ed",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">1Ô∏è‚É£ Why Activation Functions?</h3>\n",
    "\n",
    "Activation functions introduce **non-linearity** into neural networks. Without them, a network made of stacked linear transformations would collapse into a single linear model ‚Äî unable to learn complex patterns.\n",
    "\n",
    "**Analogy:**  \n",
    "> Imagine combining colored lenses (layers). Without activation (color change), stacking lenses does not produce new colors ‚Äî only scaling. Activation functions are the filters that transform inputs into richer representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d750948d",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">2Ô∏è‚É£ Linear vs Non-linear Activations</h3>\n",
    "\n",
    "- **Linear activation (identity):** \\( f(x) = x \\) ‚Äî rarely used in hidden layers because stacking linear layers is equivalent to one linear transformation.\n",
    "- **Non-linear activations:** Allow the network to learn complex decision boundaries and hierarchical features.\n",
    "\n",
    "**Conclusion:** Use non-linear activations in hidden layers to enable deep learning power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1643a34b",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">3Ô∏è‚É£ Sigmoid Function</h3>\n",
    "\n",
    "**Formula:** \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\)\n",
    "\n",
    "**Properties:**\n",
    "- Output range: (0, 1)  \n",
    "- Smooth and differentiable  \n",
    "- Historically used for binary classification output\n",
    "\n",
    "**Limitations:**\n",
    "- **Vanishing gradients** for large |x| (gradients near 0)  \n",
    "- Not zero-centered (can slow convergence)\n",
    "\n",
    "**When to use:** Output layer for binary classification (though `sigmoid + BCE` is common).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7a379d",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">4Ô∏è‚É£ Tanh Function</h3>\n",
    "\n",
    "**Formula:** \\( \\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \\)\n",
    "\n",
    "**Properties:**\n",
    "- Output range: (-1, 1)  \n",
    "- Zero-centered ‚Äî often trains faster than sigmoid\n",
    "- Still suffers from vanishing gradients for large |x|\n",
    "\n",
    "**When to use:** Hidden layers when zero-centered outputs are beneficial; less common now compared to ReLU for deep nets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4612d4b",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">5Ô∏è‚É£ ReLU (Rectified Linear Unit)</h3>\n",
    "\n",
    "**Formula:** \\( \\text{ReLU}(x) = \\max(0, x) \\)\n",
    "\n",
    "**Properties:**\n",
    "- Output range: [0, ‚àû)  \n",
    "- Simple & computationally efficient  \n",
    "- Helps mitigate vanishing gradient (for x > 0)  \n",
    "- Encourages sparse activations (many zeros)\n",
    "\n",
    "**Limitations:** Dying ReLU problem ‚Äî neurons can become inactive if weights lead to negative inputs consistently.\n",
    "\n",
    "**When to use:** Default choice for hidden layers in most modern architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32561dfc",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">6Ô∏è‚É£ Leaky ReLU & Parametric ReLU</h3>\n",
    "\n",
    "**Leaky ReLU formula:** \\( f(x) = x \\) if x > 0 else \\( 0.01x \\) (slope for negative side)\n",
    "\n",
    "**Parametric ReLU (PReLU):** Negative slope is learned during training.\n",
    "\n",
    "**Benefits:**\n",
    "- Prevents neurons from dying completely  \n",
    "- Keeps small gradient for negative inputs\n",
    "\n",
    "**When to use:** If ReLU causes dead neurons or you want a small negative slope for robustness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ab7f1",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">7Ô∏è‚É£ Softmax ‚Äî Multi-class Output</h3>\n",
    "\n",
    "**Formula (for class i):** \\( \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} \\)\n",
    "\n",
    "**Properties:**\n",
    "- Converts logits to probability distribution (sum = 1)  \n",
    "- Use in multi-class classification output layer with categorical cross-entropy loss\n",
    "\n",
    "**When to use:** Output layer for multi-class problems (e.g., 10-class digit classification).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbb6209",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">8Ô∏è‚É£ SELU & ELU (Advanced activations)</h3>\n",
    "\n",
    "- **ELU (Exponential Linear Unit):** Smooth and can produce negative values which helps with mean activations near zero.  \n",
    "- **SELU (Scaled ELU):** Designed for self-normalizing networks (works well with specific initializations and architectures).\n",
    "\n",
    "**Use-case:** Advanced architectures and research; not first-choice for every beginner project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858d7b59",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">9Ô∏è‚É£ Activation Comparison Table</h3>\n",
    "\n",
    "| Activation | Range | Pros | Cons | Typical Use |\n",
    "|-----------|-------|------|------|-------------|\n",
    "| Sigmoid | (0,1) | Smooth, interpretable | Vanishing gradient, not zero-centered | Binary output |\n",
    "| Tanh | (-1,1) | Zero-centered | Vanishing gradient | Hidden layers (some cases) |\n",
    "| ReLU | [0,‚àû) | Fast, sparse activations | Dying ReLU | Hidden layers (default) |\n",
    "| Leaky ReLU | (-‚àû,‚àû) | Avoids dead neurons | Slightly more computation | Hidden layers |\n",
    "| Softmax | (0,1) per class | Probabilities across classes | Requires numeric stability care | Multi-class output |\n",
    "| ELU/SELU | (-‚àû,‚àû) | Better negative saturation behavior | More compute/complex | Research/advanced |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a0b5f4",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">üîü Choosing the Right Activation ‚Äî Practical Tips</h3>\n",
    "\n",
    "- **Hidden layers:** Start with **ReLU** (or Leaky ReLU if dead neurons appear).  \n",
    "- **Binary classification output:** **Sigmoid** with Binary Cross-Entropy loss.  \n",
    "- **Multi-class classification output:** **Softmax** with Categorical Cross-Entropy loss.  \n",
    "- **Regression (real-valued outputs):** Linear activation (no activation) on output layer.  \n",
    "- **If training is unstable:** Try Batch Normalization, smaller learning rates, or Leaky ReLU/ELU.\n",
    "\n",
    "**Rule of thumb:** ReLU for hidden layers; choice of output activation depends on problem type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15307738",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">‚úÖ Summary & Next Steps</h3>\n",
    "\n",
    "- Activation functions are essential for non-linearity and model capacity.  \n",
    "- ReLU is the default for hidden layers; Softmax/Sigmoid for outputs depending on task.  \n",
    "- Be aware of vanishing gradients and dead neurons; pick activations and regularization accordingly.\n",
    "\n",
    "**Next:** Proceed to `05_Loss_Functions_&_Optimizers/` for detailed explanations of loss functions (MSE, Cross-Entropy) and optimizers (SGD, Adam, RMSProp).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
