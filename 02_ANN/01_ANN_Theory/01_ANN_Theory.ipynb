{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5af07b",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;\">Artificial Neural Network (ANN) ‚Äî Theory</h2>\n",
    "\n",
    "**Author:** Mubasshir Ahmed  \n",
    "**Module:** Deep Learning ‚Äî FSDS  \n",
    "**Notebook:** 01_ANN_Theory  \n",
    "**Objective:** Understand the intuition, structure, and working principle of Artificial Neural Networks (ANN).\n",
    "\n",
    "---\n",
    "\n",
    "Artificial Neural Networks (ANNs) are the foundation of Deep Learning. They are designed to **simulate how the human brain learns** and processes information ‚Äî through a network of interconnected ‚Äúneurons.‚Äù\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b8219",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">1Ô∏è‚É£ What is an Artificial Neural Network?</h3>\n",
    "\n",
    "An **Artificial Neural Network (ANN)** is a computational model inspired by the human brain.  \n",
    "It consists of **layers of neurons (nodes)** that process data through mathematical operations.\n",
    "\n",
    "Each neuron receives inputs, applies a **weighted transformation**, adds a **bias**, passes the result through an **activation function**, and sends the output to the next layer.\n",
    "\n",
    "**Mathematical Form:**  \n",
    "\\[ z = (w_1x_1 + w_2x_2 + ... + w_nx_n) + b \\]  \n",
    "\\[ a = f(z) \\]\n",
    "\n",
    "Where:  \n",
    "- **x** ‚Üí inputs  \n",
    "- **w** ‚Üí weights  \n",
    "- **b** ‚Üí bias  \n",
    "- **f(z)** ‚Üí activation function output  \n",
    "- **a** ‚Üí neuron output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654fd469",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">2Ô∏è‚É£ Biological Neuron vs Artificial Neuron</h3>\n",
    "\n",
    "| Biological Neuron | Artificial Neuron |\n",
    "|--------------------|--------------------|\n",
    "| Dendrites receive signals | Inputs (x‚ÇÅ, x‚ÇÇ, ‚Ä¶, xn) received |\n",
    "| Soma processes signals | Weighted sum computed |\n",
    "| Axon transmits output | Activation function output |\n",
    "| Synapses connect neurons | Weights connect layers |\n",
    "\n",
    "**Analogy:**  \n",
    "> Think of each neuron as a small decision-maker that contributes to the final output.  \n",
    "> Just like our brain strengthens connections through learning, ANNs adjust their weights over time to minimize errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0d219e",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">3Ô∏è‚É£ Structure of an ANN</h3>\n",
    "\n",
    "A basic ANN consists of three main types of layers:\n",
    "\n",
    "1. **Input Layer** ‚Üí Takes in raw data features (e.g., pixels, columns in dataset).  \n",
    "2. **Hidden Layers** ‚Üí Perform intermediate computations and feature extraction.  \n",
    "3. **Output Layer** ‚Üí Produces the final result (classification, regression, etc.).\n",
    "\n",
    "Each connection between neurons has a **weight** that determines its importance.\n",
    "\n",
    "**Key Terms:**\n",
    "- **Weights:** The strength of a connection between neurons.  \n",
    "- **Bias:** Allows flexibility in learning decision boundaries.  \n",
    "- **Activation Function:** Introduces non-linearity, helping the network learn complex patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb53d9c",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">4Ô∏è‚É£ Feedforward Process ‚Äî How ANN Predicts</h3>\n",
    "\n",
    "In the **feedforward** phase, information moves only in one direction ‚Äî from input to output.\n",
    "\n",
    "1. Input data passes into the input layer.  \n",
    "2. Each neuron computes a weighted sum of its inputs and adds bias.  \n",
    "3. The activation function decides the neuron‚Äôs output.  \n",
    "4. The process continues layer by layer until reaching the output.\n",
    "\n",
    "**Formula:**  \n",
    "\\[ a^{(l)} = f(W^{(l)}a^{(l-1)} + b^{(l)}) \\]\n",
    "\n",
    "This helps the network produce predictions (≈∑) for given input data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a82882",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">5Ô∏è‚É£ Backpropagation ‚Äî How ANN Learns</h3>\n",
    "\n",
    "After making predictions, the ANN must **learn** by comparing predicted vs actual outputs.\n",
    "\n",
    "This happens through **backpropagation**, where the model adjusts its weights based on the **error (loss)**.\n",
    "\n",
    "**Steps:**\n",
    "1. Compute error using a **loss function** (e.g., MSE, Cross-Entropy).  \n",
    "2. Calculate **gradients** of the error w.r.t each weight using **chain rule**.  \n",
    "3. Update weights to reduce the error using **Gradient Descent**.\n",
    "\n",
    "**Gradient Descent Update Rule:**  \n",
    "\\[ W_{new} = W_{old} - \\eta \\frac{‚àÇL}{‚àÇW} \\]\n",
    "\n",
    "Where:  \n",
    "- **Œ∑ (eta)** = learning rate  \n",
    "- **‚àÇL/‚àÇW** = derivative of loss w.r.t weight  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff20e8",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">6Ô∏è‚É£ Key Parameters in ANN Training</h3>\n",
    "\n",
    "| Parameter | Meaning | Example Value |\n",
    "|------------|----------|----------------|\n",
    "| **Learning Rate (Œ∑)** | Step size for weight updates | 0.01, 0.001 |\n",
    "| **Epoch** | One full pass of dataset through network | 100 |\n",
    "| **Batch Size** | Samples per weight update | 32, 64 |\n",
    "| **Loss Function** | Measures prediction error | MSE, CrossEntropy |\n",
    "| **Optimizer** | Algorithm that updates weights | Adam, SGD |\n",
    "\n",
    "**Tip:** A smaller learning rate leads to slow but stable training; too large can cause oscillation or divergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee622d4d",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">7Ô∏è‚É£ Single-Layer vs Multi-Layer Neural Network</h3>\n",
    "\n",
    "| Type | Description | Capability |\n",
    "|------|-------------|-------------|\n",
    "| **Single-Layer (Perceptron)** | Only one input & output layer | Can only solve linearly separable problems |\n",
    "| **Multi-Layer Neural Network (MLP)** | Has one or more hidden layers | Can learn non-linear complex patterns |\n",
    "\n",
    "> Adding more layers increases learning power ‚Äî but also increases risk of overfitting and computation cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e6154",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">8Ô∏è‚É£ Challenges in ANN</h3>\n",
    "\n",
    "| Problem | Description | Mitigation |\n",
    "|----------|--------------|-------------|\n",
    "| **Overfitting** | Model memorizes training data instead of generalizing | Dropout, Regularization, Cross-Validation |\n",
    "| **Vanishing Gradient** | Gradients become too small to update weights | ReLU, BatchNorm, Gradient Clipping |\n",
    "| **Underfitting** | Model too simple to learn data | Add layers or neurons |\n",
    "| **Slow Training** | Large models take time to converge | Use GPUs, better optimizers (Adam, RMSProp) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad8b551",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">9Ô∏è‚É£ ANN Workflow Summary</h3>\n",
    "\n",
    "**Step-by-Step Learning Flow:**\n",
    "\n",
    "1. Input raw data ‚Üí normalized or scaled  \n",
    "2. Forward pass ‚Üí calculate predictions  \n",
    "3. Compute loss ‚Üí using target vs predicted output  \n",
    "4. Backward pass ‚Üí adjust weights via gradients  \n",
    "5. Repeat for multiple epochs until loss stabilizes  \n",
    "\n",
    "**In short:**  \n",
    "> Feedforward predicts ‚Üí Backpropagation learns ‚Üí Optimizer improves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cec895",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">üîü Summary & Key Takeaways</h3>\n",
    "\n",
    "- ANN is inspired by how the human brain processes information.  \n",
    "- It learns by **forward propagation** (predict) and **backpropagation** (learn).  \n",
    "- Weights and biases control how neurons interact.  \n",
    "- Activation functions introduce **non-linearity**.  \n",
    "- Multi-layer networks can learn **complex patterns**.  \n",
    "- Overfitting and vanishing gradients are major training challenges.\n",
    "\n",
    "**Next:** Move to `02_Neural_Network_Architecture/` to understand layer design and connections in depth.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
