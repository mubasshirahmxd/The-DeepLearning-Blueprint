{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e34a3a5",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;\">Forward and Backward Propagation</h2>\n",
    "\n",
    "**Author:** Mubasshir Ahmed  \n",
    "**Module:** Deep Learning ‚Äî FSDS  \n",
    "**Notebook:** 03_Forward_Backward_Propagation  \n",
    "**Objective:** To understand the complete mechanism of how data flows forward through a neural network and how the network learns through backward propagation.\n",
    "\n",
    "---\n",
    "\n",
    "A Neural Network learns by performing two key steps repeatedly:\n",
    "\n",
    "1. **Forward Propagation** ‚Äî Generates predictions.  \n",
    "2. **Backward Propagation** ‚Äî Learns from mistakes by adjusting weights.\n",
    "\n",
    "Together, these steps form the **core learning cycle** of Deep Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d10b35",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">1Ô∏è‚É£ What is Forward Propagation?</h3>\n",
    "\n",
    "**Forward Propagation** is the process of moving data from the **input layer ‚Üí hidden layers ‚Üí output layer** to generate predictions.\n",
    "\n",
    "At each layer, neurons perform a weighted computation and apply an activation function.\n",
    "\n",
    "**Mathematical Flow (for one neuron):**\n",
    "\\[ z = (w_1x_1 + w_2x_2 + ... + w_nx_n) + b \\]\n",
    "\\[ a = f(z) \\]\n",
    "\n",
    "Where:\n",
    "- **x·µ¢** = input features  \n",
    "- **w·µ¢** = weights  \n",
    "- **b** = bias  \n",
    "- **f** = activation function  \n",
    "- **a** = activated output\n",
    "\n",
    "The output of one layer becomes the input for the next, until we reach the final prediction \\( \\hat{y} \\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226b827",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">2Ô∏è‚É£ Example ‚Äî Feedforward Intuition</h3>\n",
    "\n",
    "Let‚Äôs imagine a simple network predicting whether a student passes (1) or fails (0) based on hours studied and sleep hours.\n",
    "\n",
    "| Input | Hidden Layer | Output |\n",
    "|--------|---------------|---------|\n",
    "| Hours Studied, Sleep Hours | Weighted sums ‚Üí Activation | Probability of Pass |\n",
    "\n",
    "1. Input data (2 features) is fed to neurons.  \n",
    "2. Each neuron applies weight & bias.  \n",
    "3. Activation (ReLU/Sigmoid) determines its output.  \n",
    "4. The final neuron predicts \\( \\hat{y} = 0.92 \\) ‚Üí means ‚Äú92% chance of passing.‚Äù\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c0df1",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">3Ô∏è‚É£ The Mathematics Behind Forward Pass</h3>\n",
    "\n",
    "For a 3-layer neural network:\n",
    "\n",
    "\\[ a^{(1)} = X \\]\n",
    "\\[ z^{(2)} = W^{(2)} a^{(1)} + b^{(2)} \\]\n",
    "\\[ a^{(2)} = f(z^{(2)}) \\]\n",
    "\\[ z^{(3)} = W^{(3)} a^{(2)} + b^{(3)} \\]\n",
    "\\[ a^{(3)} = f(z^{(3)}) = \\hat{y} \\]\n",
    "\n",
    "Here, \\( \\hat{y} \\) is the predicted output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c61ff0f",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">4Ô∏è‚É£ Loss Function ‚Äî Measuring the Error</h3>\n",
    "\n",
    "After obtaining predictions (\\( \\hat{y} \\)), the model calculates **how far off** it was from the true output (\\( y \\)).\n",
    "\n",
    "This difference is captured using a **Loss Function** (or Cost Function).\n",
    "\n",
    "| Problem Type | Common Loss Function | Formula |\n",
    "|---------------|----------------------|----------|\n",
    "| Regression | Mean Squared Error (MSE) | \\( L = \\frac{1}{n}\\sum(y - \\hat{y})^2 \\) |\n",
    "| Binary Classification | Binary Cross-Entropy | \\( L = -[y \\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})] \\) |\n",
    "| Multi-class Classification | Categorical Cross-Entropy | Similar extension of BCE |\n",
    "\n",
    "> Lower loss = better model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ab786",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">5Ô∏è‚É£ What is Backward Propagation?</h3>\n",
    "\n",
    "**Backward Propagation (Backprop)** is the process of **updating the weights** of the neural network by propagating the loss backward from the output layer to the input layer.\n",
    "\n",
    "It‚Äôs based on **Calculus Chain Rule** ‚Äî it computes how much each weight contributed to the total error.\n",
    "\n",
    "**Intuition:**  \n",
    "> Think of it as feedback ‚Äî if the network made a mistake, it traces back to see which neurons caused it and adjusts them accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d77f0",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">6Ô∏è‚É£ Steps in Backpropagation</h3>\n",
    "\n",
    "1. **Compute Loss:** Compare predicted output \\( \\hat{y} \\) with actual output \\( y \\).  \n",
    "2. **Compute Gradient:** Calculate partial derivatives (‚àÇL/‚àÇW) using Chain Rule.  \n",
    "3. **Update Weights:** Move weights in the direction that reduces error.  \n",
    "4. **Repeat:** Do this for all training samples across multiple epochs.\n",
    "\n",
    "**Weight Update Rule:**  \n",
    "\\[ W_{new} = W_{old} - \\eta \\frac{\\partial L}{\\partial W} \\]\n",
    "\n",
    "Where:  \n",
    "- \\( \\eta \\) = learning rate  \n",
    "- \\( \\frac{\\partial L}{\\partial W} \\) = gradient (rate of change of loss w.r.t weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee19d783",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">7Ô∏è‚É£ Chain Rule ‚Äî The Core of Backpropagation</h3>\n",
    "\n",
    "The **chain rule** connects derivatives across layers:\n",
    "\n",
    "\\[ \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial a} \\times \\frac{\\partial a}{\\partial z} \\times \\frac{\\partial z}{\\partial W} \\]\n",
    "\n",
    "This means:\n",
    "- If one neuron makes an error, the gradient tells us **how much** that neuron (and its weights) contributed.  \n",
    "- The process continues **layer by layer backward** until all weights are adjusted.\n",
    "\n",
    "Hence the name ‚Äî **backward propagation of errors**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8832091",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">8Ô∏è‚É£ Gradient Descent ‚Äî How Learning Happens</h3>\n",
    "\n",
    "**Gradient Descent** is an optimization algorithm that minimizes loss by adjusting weights iteratively.\n",
    "\n",
    "At each step, it moves the weights **slightly opposite** to the gradient direction (downhill on loss curve).\n",
    "\n",
    "**Update Formula:**  \n",
    "\\[ W := W - \\eta \\frac{\\partial L}{\\partial W} \\]\n",
    "\n",
    "| Type | Description |\n",
    "|------|--------------|\n",
    "| **Batch Gradient Descent** | Uses all data at once for one update |\n",
    "| **Stochastic Gradient Descent (SGD)** | Updates weights after every sample |\n",
    "| **Mini-batch GD** | Updates weights after a small batch (most common) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bdd1b3",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">9Ô∏è‚É£ Learning Rate, Epochs & Iterations</h3>\n",
    "\n",
    "- **Learning Rate (Œ∑):** Controls the step size of updates.  \n",
    "  - Too high ‚Üí overshoot minima (unstable)  \n",
    "  - Too low ‚Üí very slow convergence  \n",
    "\n",
    "- **Epoch:** One full pass of dataset through network.  \n",
    "- **Iteration:** One update step (per batch).\n",
    "\n",
    "**Example:**  \n",
    "If you have 1000 samples, batch size = 100 ‚Üí 10 iterations = 1 epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0bda8a",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">üîü Full Learning Cycle ‚Äî Putting It All Together</h3>\n",
    "\n",
    "1. Input data is fed into the network.  \n",
    "2. **Forward Propagation** computes predictions.  \n",
    "3. **Loss Function** measures the prediction error.  \n",
    "4. **Backward Propagation** computes gradients.  \n",
    "5. **Optimizer (e.g., Adam, SGD)** updates weights.  \n",
    "6. Repeat across many epochs until convergence.\n",
    "\n",
    "**Visualization:**  \n",
    "> Forward pass = making predictions.  \n",
    "> Backward pass = correcting mistakes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e68a3d",
   "metadata": {},
   "source": [
    "### <h3 style=\"text-align:center;\">üß† Summary & Key Takeaways</h3>\n",
    "\n",
    "- Forward propagation generates predictions from input data.  \n",
    "- Backpropagation corrects errors by adjusting weights.  \n",
    "- The **loss function** quantifies how wrong the network is.  \n",
    "- **Gradient Descent** minimizes the loss by updating parameters.  \n",
    "- Repeating this process over epochs = model learning.\n",
    "\n",
    "**Next:** Proceed to `04_Activation_Functions/` to understand how activations bring non-linearity into neural networks.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
